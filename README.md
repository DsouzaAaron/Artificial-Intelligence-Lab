AI Lab - Problem Solving Overview
This README provides an overview of the various AI problems tackled over a 10-week period in the AI Lab. Each week focused on a specific area of AI, ranging from representational learning to reinforcement learning and optimization. Below is a detailed description of the problems solved each week.

WEEK 1: Representational Learning using Autoencoders
In the first week, we delved into autoencoders, a type of neural network used for unsupervised learning. The focus was on understanding how autoencoders can compress data into a lower-dimensional representation and then reconstruct it. We explored various architectures, including convolutional and variational autoencoders, and evaluated their performance in reconstructing input data.

WEEK 2: Simple Agents
During the second week, we studied simple AI agents capable of performing basic tasks. This involved understanding the concept of agents and their environments. We implemented simple reflex agents and model-based agents, testing their behavior in different scenarios to observe how they make decisions based on predefined rules or models of the environment.

WEEK 3: Problem Solving Agents & SEARCH
In the third week, we focused on problem-solving agents using search algorithms. We covered both uninformed search algorithms like Breadth-First Search (BFS) and Depth-First Search (DFS), as well as informed search algorithms like A* and Greedy Best-First Search. These algorithms were applied to classic problems such as maze navigation and puzzle solving, demonstrating how agents can find solutions efficiently.

WEEK 4: Gaming Agent & Negamax Search
The fourth week was dedicated to developing a gaming agent using the Negamax search algorithm, a variant of the minimax algorithm used in game theory. We implemented the Negamax algorithm and enhanced it with alpha-beta pruning to optimize performance. The gaming agent was tested in various games, such as tic-tac-toe and chess, showcasing its ability to make strategic decisions.

WEEK 5: Multi-Armed Bandits in TF-Agents
In the fifth week, we explored the multi-armed bandit problem using TF-Agents, a library for reinforcement learning in TensorFlow. We implemented different strategies, such as epsilon-greedy, Upper Confidence Bound (UCB), and Thompson sampling, to solve the bandit problem. Each strategy's performance was analyzed in different contexts to understand their strengths and weaknesses.

WEEK 6: Multi-Armed Bandits – Ad Optimization
The sixth week focused on applying multi-armed bandit algorithms to ad optimization problems. We studied how bandit algorithms can optimize ad placements by balancing exploration and exploitation. Various strategies were implemented and tested to improve ad performance metrics, demonstrating the practical application of bandits in marketing.

WEEK 7: Multi-Armed Bandits – Movie Recommendation
In the seventh week, we applied multi-armed bandit algorithms to movie recommendation systems. We explored how these algorithms can improve recommendations by dynamically learning user preferences. By implementing and evaluating different bandit strategies, we aimed to enhance the accuracy and relevance of movie recommendations.

WEEK 8: MDP & Dynamic Programming
The eighth week was dedicated to solving Markov Decision Processes (MDPs) using dynamic programming techniques. We covered the fundamentals of MDPs and implemented value iteration and policy iteration algorithms. These methods were used to solve various MDP problems, highlighting their effectiveness in finding optimal policies through iterative updates.

WEEK 9: MDP & Monte Carlo Methods
During the ninth week, we applied Monte Carlo methods to solve MDPs. We focused on Monte Carlo prediction and control algorithms, which rely on averaging sample returns to estimate value functions. By solving MDP problems using these methods, we demonstrated their ability to learn from sampled experiences and improve decision-making over time.

WEEK 10: Temporal Difference – SARSA, Q-Learning
In the final week, we implemented temporal difference learning algorithms, specifically SARSA and Q-learning. These algorithms combine the ideas of Monte Carlo methods and dynamic programming, updating value estimates based on observed transitions. We compared the performance of SARSA and Q-learning in various tasks, fine-tuning the algorithms to enhance their learning efficiency and effectiveness.
