{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK 5: MULTI ARMED BANDITS IN TF-AGENTS"
      ],
      "metadata": {
        "id": "jsk4I9zoYt_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reg No: 200968008<br>\n",
        "Name: Aaron Dsouza"
      ],
      "metadata": {
        "id": "ylbx2yc9xl4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the required package\n",
        "!pip install tf-agents"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: tf-agents in /usr/local/lib/python3.8/dist-packages (0.15.0)\nRequirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.5.0)\nRequirement already satisfied: tensorflow-probability>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (4.5.0)\nRequirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from tf-agents) (8.4.0)\nRequirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (3.19.6)\nRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\nRequirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (2.1.0)\nRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.22.4)\nRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\nRequirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (2.2.1)\nRequirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.23.0)\nRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.4.0)\nRequirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (6.0.0)\nRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\nRequirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (4.4.2)\nRequirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.4.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.1.8)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.15.0)\n"
        }
      ],
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4NuZNmCYusM",
        "outputId": "6b34409b-cce1-4a1c-b858-57dee50c7d33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents import tf_agent\n",
        "from tf_agents.drivers import driver\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.trajectories import policy_step\n",
        "\n",
        "nest = tf.nest"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "id": "R2b9Zytuad2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Exercise 1 -Create a  environment \n",
        "<ul>\n",
        "<li>a. for  which  the  observation  is  a  random  integer  between -5  and  5,  there  are  3 possible actions (0, 1, 2), and the reward is the product of the action and the observation.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "369h6iRXY4kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BanditPyEnvironment(py_environment.PyEnvironment):\n",
        "  def __init__(self, observation_spec, action_spec):\n",
        "    self._observation_spec = observation_spec\n",
        "    self._action_spec = action_spec\n",
        "    super(BanditPyEnvironment, self).__init__()\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _empty_observation(self):\n",
        "    return tf.nest.map_structure(lambda x: np.zeros(x.shape, x.dtype),\n",
        "                                self.observation_spec())\n",
        "    \n",
        "  def _reset(self):\n",
        "    return ts.restart(self._observe(), batch_size = self.batch_size)\n",
        "\n",
        "\n",
        "  def _step(self, action):\n",
        "    reward = self._apply_action(action)\n",
        "    return ts.termination(self._observe(), reward)\n",
        "\n",
        "  # @abc.abstractmethod\n",
        "  #   def _observe(self):\n",
        "\n",
        "\n",
        "  # @abc.abstractmethod\n",
        "  #   def _apply_action(self,action):"
      ],
      "outputs": [],
      "execution_count": 38,
      "metadata": {
        "id": "cE3PvTf4Y8Do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimplePyEnvironment(BanditPyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype = np.int32, minimum=0, maximum=2, name='action'\n",
        "    )\n",
        "    observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=-5, maximum=5, name='observation'\n",
        "    )\n",
        "    super(SimplePyEnvironment, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "  # This function returns a random observation between -5 to 5\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-5,6,(1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  # This function calculates and returns the reward for the specific observation\n",
        "  def _apply_action(self, action):\n",
        "    return action * self._observation"
      ],
      "outputs": [],
      "execution_count": 39,
      "metadata": {
        "id": "Kc8sRw5XbGy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a BanditPYEnvironment\n",
        "environment = SimplePyEnvironment()\n",
        "observation = environment.reset().observation\n",
        "print(\"observation: %d\" % observation)\n",
        "\n",
        "action = np.random.randint(0,3,(1,), dtype='int32')\n",
        "\n",
        "print(\"action: %d\" % action)\n",
        "reward = environment.step(action).reward\n",
        "print(\"reward: %f\" % reward)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "observation: -5\naction: 1\nreward: -5.000000\n"
        }
      ],
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_3ItdfAbJuF",
        "outputId": "8c89a4b0-4c07-46c4-f12c-6fede1af1ee4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<ul>\n",
        "<li>b. Define an optimal policy manually. The action only depends on the sign of the observation, 0 when is negative and 2 when is positive.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "l79ftcFXcoY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapping the BanditPyEnvironment defined before with TFPyEnvironment\n",
        "tf_environment = tf_py_environment.TFPyEnvironment(environment)"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {
        "id": "Ju3Z7jq4c0nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SignPolicy(tf_policy.TFPolicy):\n",
        "  def __init__(self):\n",
        "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(1,), dtype=tf.int32, minimum=-5, maximum=5)\n",
        "    time_step_spec = ts.time_step_spec(observation_spec)\n",
        "\n",
        "    action_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(), dtype = tf.int32, minimum=0, maximum=2)\n",
        "\n",
        "    super(SignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
        "                                     action_spec =action_spec)\n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  def _variables(self):\n",
        "    return ()\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    observation_sign = tf.cast(tf.sign(time_step.observation), dtype=tf.int32)\n",
        "    action = observation_sign + 1\n",
        "    # return policy_step.PolicyStep(action, policy_step) this causes output structure mismatch\n",
        "    return policy_step.PolicyStep(action, ())"
      ],
      "outputs": [],
      "execution_count": 42,
      "metadata": {
        "id": "ukl10jUYdqr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sign_policy = SignPolicy()\n",
        "\n",
        "current_time_step = tf_environment.reset()\n",
        "print('Observation:')\n",
        "# print (current_time_step.observation)\n",
        "action = sign_policy.action(current_time_step).action\n",
        "print('Action:')\n",
        "print (action)\n",
        "reward = tf_environment.step(action).reward\n",
        "print('Reward:')\n",
        "print(reward)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Observation:\nAction:\ntf.Tensor([[2]], shape=(1, 1), dtype=int32)\nReward:\ntf.Tensor([[8.]], shape=(1, 1), dtype=float32)\n"
        }
      ],
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJnhIjB3eXdG",
        "outputId": "3b50dc79-971c-4c3c-b799-7930a1892def"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<ul><li>c. Request  for  50  observations  from  the  environment,  compute  and  print  the total reward.</li></ul>"
      ],
      "metadata": {
        "id": "3cxCh23xfTJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step = tf_environment.reset()\n",
        "total_reward = 0\n",
        "for _ in range(50):\n",
        "  action_step = sign_policy.action(step).action\n",
        "  reward = tf_environment.step(action_step).reward\n",
        "  next_step = tf_environment.step(action_step)\n",
        "  total_reward += reward\n",
        "  step = next_step\n",
        "\n",
        "print(\"Total Reward\",np.array(total_reward)[0])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Total Reward [124.]\n"
        }
      ],
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx-k2nfvfY-I",
        "outputId": "456554db-a883-4b19-b88c-5df612a81fc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Exercise 2 –Create an environment \n",
        "<ul>\n",
        "<li>a. Define an environment will either always give reward = observation * action or reward = -observation * action. This will be decided when the environment is initialized.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "jl_Ett-aZrkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining an Environment\n",
        "class TwoWayPyEnvironment(BanditPyEnvironment):\n",
        "  def __init__(self):\n",
        "    action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='Action'\n",
        "    )\n",
        "    observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=-5, maximum=5, name='Observation'\n",
        "    )\n",
        "    self._reward_sign = 2*np.random.randint(2)-1\n",
        "    print(\"reward sign: \",self._reward_sign)\n",
        "    super(TwoWayPyEnvironment, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-5,6,(1,),dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  def _apply_action(self, action):\n",
        "    return self._reward_sign*action*self._observation[0]"
      ],
      "outputs": [],
      "execution_count": 45,
      "metadata": {
        "id": "FYaNbbKfaBtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "two_way_tf_environment = tf_py_environment.TFPyEnvironment(TwoWayPyEnvironment())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "reward sign:  -1\n"
        }
      ],
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tlus32_wODli",
        "outputId": "2cd15832-b064-4573-effe-58be532cbdda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<ul>\n",
        "<li>b. Define a policy that detects the behaviorof the underlying environment. There are three situations that the policy needs to handle:\n",
        "<ul>\n",
        "<li>i.The agent has not detected know yet which version of the environment is running.</li>\n",
        "<li>ii.The  agent  detected  that  the  original  version  of  the  environment  is running.</li>\n",
        "<li>iii.The  agent  detected  that  the  flipped  version  of  the  environment  is running.</li>\n",
        "</ul></li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "aAl4AJH-tG5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoWaySignPolicy(tf_policy.TFPolicy):\n",
        "  def __init__(self, situation):\n",
        "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(1,), dtype=tf.int32, minimum=-5, maximum=5\n",
        "    )\n",
        "    action_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(), dtype=tf.int32, minimum=0, maximum=2\n",
        "    )\n",
        "    time_step_spec = ts.time_step_spec(observation_spec)\n",
        "    self._situation = situation\n",
        "    super(TwoWaySignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
        "                                           action_spec=action_spec)\n",
        "  \n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  def _variables(self):\n",
        "    return [self._situation]\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    sign = tf.cast(tf.sign(time_step.observation[0,0]), dtype=tf.int32)\n",
        "\n",
        "    # Case-1: The agent has not detected know yet which version of the environment is running.\n",
        "    def case_unknown_fn():\n",
        "      return tf.constant(1, shape=(1,))\n",
        "\n",
        "    # Case-2: The agent detected that the original version of the environment is running.\n",
        "    def case_normal_fn():\n",
        "      return tf.constant(sign+1, shape=(1,))\n",
        "\n",
        "    # Case-3: The agent detected that the flipped version of the environment is running.\n",
        "    def case_flipped_fn():\n",
        "      return tf.constant(1-sign, shape=(1,))\n",
        "\n",
        "    cases = [(tf.equal(self._situation, 0), case_unknown_fn),\n",
        "             (tf.equal(self._situation, 1), case_normal_fn),\n",
        "             (tf.equal(self._situation, 2), case_flipped_fn)]\n",
        "\n",
        "    action = tf.case(cases, exclusive=True)\n",
        "    return policy_step.PolicyStep(action, policy_state)\n",
        "              \n"
      ],
      "outputs": [],
      "execution_count": 47,
      "metadata": {
        "id": "ncng4I74OhqO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": 47,
      "metadata": {
        "id": "5-xJe8Tkt4cI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<ul>\n",
        "<li>c. Define the agent that detects the sign of the environment and sets the policy appropriately.\n",
        "</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "q-HyI-q3u4MN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Agent\n",
        "# variable 'situation' is shared by the agent and the policy.\n",
        "class SignAgent(tf_agent.TFAgent):\n",
        "  def __init__(self):\n",
        "    self._situation = tf.Variable(0, dtype=tf.int32)\n",
        "    policy = TwoWaySignPolicy(self._situation)\n",
        "    time_step_spec = policy.time_step_spec\n",
        "    action_spec = policy.action_spec\n",
        "    super(SignAgent, self).__init__(time_step_spec=time_step_spec,\n",
        "                                    action_spec=action_spec,\n",
        "                                    policy=policy,\n",
        "                                    collect_policy=policy,\n",
        "                                    train_sequence_length=None)\n",
        "\n",
        "  def _initialize(self):\n",
        "    return tf.compat.v1.variables_initializer(self.variables)\n",
        "\n",
        "  def _train(self, experience, weights=None):\n",
        "    observation = experience.observation\n",
        "    action = experience.action\n",
        "    reward = experience.reward\n",
        "\n",
        "    needs_action = tf.logical_and(tf.equal(self._situation, 0),\n",
        "                                  tf.not_equal(reward, 0))\n",
        "\n",
        "\n",
        "    def new_situation_fn():\n",
        "      return (3 - tf.sign(tf.cast(observation[0, 0, 0], dtype=tf.int32) *\n",
        "                          tf.cast(action[0, 0], dtype=tf.int32) *\n",
        "                          tf.cast(reward[0, 0], dtype=tf.int32))) / 2\n",
        "\n",
        "    new_situation = tf.cond(needs_action,\n",
        "                            new_situation_fn,\n",
        "                            lambda: self._situation)\n",
        "    new_situation = tf.cast(new_situation, tf.int32)\n",
        "    tf.compat.v1.assign(self._situation, new_situation)\n",
        "    return tf_agent.LossInfo((), ())\n",
        "\n",
        "sign_agent = SignAgent()"
      ],
      "outputs": [],
      "execution_count": 48,
      "metadata": {
        "id": "gUgiM2p0SEne"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": 48,
      "metadata": {
        "id": "M_H5dQRGuO7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trajectories are tuples that contain samples taken from the previous steps. These samples are then used by the agent to train and update the policy."
      ],
      "metadata": {
        "id": "xGP-kLDDvnol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trajectory_for_bandit(initial_step, action_step, final_step):\n",
        "  return trajectory.Trajectory(observation=tf.expand_dims(initial_step.observation, 0),\n",
        "                               action=tf.expand_dims(action_step.action, 0),\n",
        "                               policy_info=action_step.info,\n",
        "                               reward=tf.expand_dims(final_step.reward, 0),\n",
        "                               discount=tf.expand_dims(final_step.discount, 0),\n",
        "                               step_type=tf.expand_dims(initial_step.step_type, 0),\n",
        "                               next_step_type=tf.expand_dims(final_step.step_type, 0))"
      ],
      "outputs": [],
      "execution_count": 49,
      "metadata": {
        "id": "w1ZKbHher6XG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Training the Agent</h3>"
      ],
      "metadata": {
        "id": "9_G3LU1tvxt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step = two_way_tf_environment.reset()\n",
        "total_reward=0\n",
        "for _ in range(20):\n",
        "  action_step = sign_agent.collect_policy.action(step)\n",
        "  total_reward += step.reward\n",
        "  next_step = two_way_tf_environment.step(action_step.action)\n",
        "  experience = trajectory_for_bandit(step, action_step, next_step)\n",
        "  print(experience)\n",
        "  sign_agent.train(experience)\n",
        "  step = next_step\n",
        "\n",
        "print(\"\\n\\nTotal Reward: \",np.array(total_reward)[0])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-1.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-3]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[6.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[4]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-4]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[8.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[0]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[3]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-5]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[10.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-3]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[6.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[0]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[3]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-3]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[6.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-4]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[8.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nTrajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[4]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n\n\nTotal Reward:  53.0\n"
        }
      ],
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV1BzWSVUeLe",
        "outputId": "adc3d488-6643-46be-bf3d-1e370b838dd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After some steps, we can see that the reward is always non-negative i.e, the agent detects the sign of the environment and sets the policy accordingly."
      ],
      "metadata": {
        "id": "Vq0T1-j2wkGD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": 50,
      "metadata": {
        "id": "K-jESF5BsApa"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}