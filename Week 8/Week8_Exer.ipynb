{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WEEK8 MDP & DYNAMIC PROGRAMMING"
      ],
      "metadata": {
        "id": "NBRxi2FhEFYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reg No.: 200968008<br>\n",
        "Name: Aaron Dsouza"
      ],
      "metadata": {
        "id": "ZMPtlngyENZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "# !pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfN0ykMbaTvl",
        "outputId": "1e3d76e6-0112-4c3a-b24b-a85a3065ce2b"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.1.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "jo__O00vvn8p"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Frozen lake environment\n",
        "env = gym.make(\"FrozenLake-v1\") \n",
        "\n",
        "import os\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n"
      ],
      "metadata": {
        "id": "wkThjff-vqWs"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvGqT76yNkk9",
        "outputId": "ada0a16b-4cbd-4958-fdea-8af32d5ab07b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action space:  Discrete(4)\n",
            "Observation space:  Discrete(16)\n"
          ]
        }
      ],
      "source": [
        "print(\"Action space: \", env.action_space)\n",
        "print(\"Observation space: \", env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Action Space:<br>\n",
        "Indicates which direction to move the player\n",
        "<ul>\n",
        "<li>0 - Move Left</li>\n",
        "<li>1 - Move Down</li>\n",
        "<li>2 - Move Right</li>\n",
        "<li>3 - Move Up</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "2qHIJagNv3vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action_mapping = {\n",
        "      3: '\\u2191',  # up\n",
        "      2: '\\u2192',  # right\n",
        "      1: '\\u2193',  # down\n",
        "      0: '\\u2190'   # left\n",
        "}"
      ],
      "metadata": {
        "id": "nitmA_Q12cNp"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Simple Implementation of FrozenLake Problem"
      ],
      "metadata": {
        "id": "GqDMtUkC1Hpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_episodes = 10000\n",
        "lr = 0.2\n",
        "max_steps = 100\n",
        "gamma = 0.99\n",
        "\n",
        "epsilon = 1\n",
        "max_epsilon = 1\n",
        "min_epsilon = 0.01\n",
        "decay_rate = 0.001"
      ],
      "metadata": {
        "id": "b9uvUoumaUSA"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy = np.zeros((env.observation_space.n, env.action_space.n))"
      ],
      "metadata": {
        "id": "WaBZs376z6Po"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = []\n",
        "\n",
        "for episode in range(total_episodes):\n",
        "  state = env.reset()\n",
        "  step = 0\n",
        "  done = False\n",
        "  total_reward = 0\n",
        "\n",
        "  for step in range(max_steps):\n",
        "    if random.uniform(0,1) > epsilon:\n",
        "      action = np.argmax(policy[state,:]) # Exploitation\n",
        "    else:\n",
        "      action = env.action_space.sample() # Exploration\n",
        "    \n",
        "    new_state, reward, done, info = env.step(action)\n",
        "    max_new_state = np.max(policy[new_state,:])\n",
        "    policy[state,action] += lr * (reward + gamma*max_new_state-policy[state,action])\n",
        "\n",
        "    total_reward += reward\n",
        "    state = new_state\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "  rewards.append(total_reward)\n",
        "\n",
        "print(\"Score: \",str(sum(rewards)/total_episodes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIelJqHDzK_P",
        "outputId": "b2a7832e-8d39-4611-d497-4fdf965b3ab2"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score:  0.5131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d0FSj-01tRi",
        "outputId": "a3bc703b-d751-49fa-fc10-c90e5903c8cb"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.57698226, 0.51027759, 0.51759124, 0.5157311 ],\n",
              "       [0.3308909 , 0.28984824, 0.26754308, 0.54559293],\n",
              "       [0.38027484, 0.36450218, 0.37577577, 0.47520185],\n",
              "       [0.30762211, 0.25667628, 0.31702228, 0.44409483],\n",
              "       [0.59292356, 0.45779931, 0.42083363, 0.28576982],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.4485861 , 0.08716477, 0.05565905, 0.04360167],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.39604192, 0.44090211, 0.3643306 , 0.64999149],\n",
              "       [0.24195011, 0.70810253, 0.42650758, 0.43043097],\n",
              "       [0.72787335, 0.35209284, 0.29041425, 0.21359495],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.46476266, 0.54822987, 0.78873364, 0.59589309],\n",
              "       [0.72927724, 0.92722475, 0.7590785 , 0.70698221],\n",
              "       [0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(' '.join([action_mapping[action] for action in np.argmax(policy,axis=1)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tTnbNv71dJW",
        "outputId": "64131272-8c17-4b11-e757-d3e5857ec3ab"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "← ↑ ↑ ↑ ← ← ← ← ↑ ↓ ← ← ← → ↓ ←\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1."
      ],
      "metadata": {
        "id": "VnFeAN3NLmWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Create a Policy Iteration function with the following parameters</h3>\n",
        "<ol>\n",
        "<li>policy: 2D array of a size n(S) x n(A), each cell represents a probability of taking action a in state s.\n",
        "<li>environment: Initialized OpenAI gym environment object\n",
        "<li>discount_factor: MDP discount factor\n",
        "<li>theta:  A  threshold  of  a  value  function  change.  Once  the  update  to  value function is below this number\n",
        "<li>max_iterations: Maximum number of iterations\n",
        "</ol>"
      ],
      "metadata": {
        "id": "B94APfr8Ddg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import wrappers\n",
        "from gym.envs.registration import register"
      ],
      "metadata": {
        "id": "wRDBWX_bZsix"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evalaution(env, policy, discount_factor):\n",
        "    # Initializing the value_function to zero\n",
        "    v = np.zeros(env.observation_space.n)\n",
        "\n",
        "    # Initializing some variables/terms\n",
        "    # theta = 1e-10\n",
        "    theta = 1e-5\n",
        "    i=0\n",
        "    # Update the value function until the change is below the theta value\n",
        "    while True:\n",
        "        # print(\"new iteration\\n\"+str(i++))\n",
        "        delta = 0\n",
        "        # Store the value function before updating\n",
        "        prev_v = np.copy(v)\n",
        "        # Iterate through the state space\n",
        "        for s in range(env.observation_space.n):\n",
        "            # Choose action for state s based on policy\n",
        "            policy_a = policy[s]\n",
        "            # Update the value function\n",
        "            v[s] = sum([p * (r + discount_factor * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
        "            # print(v[s])\n",
        "            delta = max(delta,(np.abs(prev_v[s] - v[s])))\n",
        "            # print(np.fabs(prev_v[s] - v[s]))\n",
        "            # print(\"Delta: \" + str(delta))\n",
        "        # print(v)\n",
        "        # Condition to break the loop\n",
        "        delta = np.sum((np.fabs(prev_v - v)))\n",
        "        if delta < theta:\n",
        "            # print(\"Converged\\n\\n\")\n",
        "            # value converged\n",
        "            break\n",
        "\n",
        "    # returning the value function\n",
        "    return v"
      ],
      "metadata": {
        "id": "-8zjv0qWyUBq"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slightly different policy evaluation function\n",
        "# def policy_evalaution(env, policy, discount_factor):\n",
        "#     # Initializing the value_function to zero\n",
        "#     v = np.zeros(env.observation_space.n)\n",
        "\n",
        "#     # Initializing some variables/terms\n",
        "#     theta = 1e-10\n",
        "#     delta = 0\n",
        "#     i=0\n",
        "#     # Update the value function until the change is below the theta value\n",
        "#     while True:\n",
        "#         # Store the value function before updating\n",
        "#         prev_v = np.copy(v)\n",
        "#         # Iterate through the state space\n",
        "#         for s in range(env.observation_space.n):\n",
        "#             # Choose action for state s based on policy\n",
        "#             policy_a = policy[s]\n",
        "#             # Update the value function\n",
        "#             v[s] = sum([p * (r + discount_factor * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
        "#         if i < 100: print(v)\n",
        "#         i +=1\n",
        "#         # Condition to break the loop\n",
        "#         delta = np.sum((np.fabs(prev_v - v)))\n",
        "#         if delta <= theta:\n",
        "#             # value converged\n",
        "#             break\n",
        "\n",
        "#     # returning the value function\n",
        "#     return v"
      ],
      "metadata": {
        "id": "mn_gD6WcdlDu"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(v, discount_factor):\n",
        "    # Initialize a policy for the state space\n",
        "    policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "    # Iterate through the state space\n",
        "    for s in range(env.observation_space.n):\n",
        "        # Initialize the Estimated action value\n",
        "        q_sa = np.zeros(env.action_space.n)\n",
        "        for a in range(env.action_space.n):\n",
        "            # Update the Initialize the Estimated action value for each action based on the value fucntion\n",
        "            q_sa[a] = sum([prb * (rew + discount_factor * v[st]) for prb, st, rew, _ in  env.P[s][a]])\n",
        "        # Update the policy based on the Estimated action value\n",
        "        policy[s] = np.argmax(q_sa)\n",
        "\n",
        "    # Return the new policy\n",
        "    return policy"
      ],
      "metadata": {
        "id": "ACLshocd24xY"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, discount_factor):\n",
        "    # Either choose a random initial policy or zero policy\n",
        "    # policy = np.random.choice(env.action_space.n, size=(env.observation_space.n))  # initialize a random policy\n",
        "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
        "\n",
        "    # Set the max no. of iterations\n",
        "    max_iterations = 1000\n",
        "    for i in range(max_iterations):\n",
        "        # Find the value function\n",
        "        value_function = policy_evalaution(env, policy, discount_factor)\n",
        "        # Obtain new policy based on the found value function\n",
        "        new_policy = policy_improvement(value_function, discount_factor)\n",
        "        # Ckeck whether the obtained policy is same as existing policy\n",
        "        if (np.all(policy == new_policy)):\n",
        "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
        "            break\n",
        "        # Update the policy\n",
        "        policy = new_policy\n",
        "    \n",
        "    # Return the optimal policy\n",
        "    return policy"
      ],
      "metadata": {
        "id": "5cMyk7ko6kTo"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episodes(env, policy, discount_factor, render = False):\n",
        "    \"\"\" Runs an episode and return the total reward \"\"\"\n",
        "    # Set number of episodes\n",
        "    num_episode = 1000\n",
        "    max_steps = 100\n",
        "    episode_reward = []\n",
        "    wins = 0\n",
        "\n",
        "    # Iterate through the episodes\n",
        "    for i in range(num_episode):\n",
        "      # Reset the Environment\n",
        "      obs = env.reset()\n",
        "      # Initialize the variables\n",
        "      total_reward = 0\n",
        "      step_idx = 0\n",
        "      while step_idx < max_steps:\n",
        "          if render:\n",
        "              env.render()\n",
        "          # Take action based on policy\n",
        "          obs, reward, done , _ = env.step(int(policy[obs]))\n",
        "          # Update the cumulative reward\n",
        "          # total_reward += (discount_factor ** step_idx * reward)\n",
        "          total_reward += reward\n",
        "          step_idx += 1\n",
        "          # Check if reached Goal State\n",
        "          if done:\n",
        "              wins += 1\n",
        "              break\n",
        "      # Add the episode reward to a List\n",
        "      episode_reward.append(total_reward)\n",
        "\n",
        "    # Return list of rewards for 1000 episodes\n",
        "    return episode_reward, wins"
      ],
      "metadata": {
        "id": "iA47S1ub6MIL"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discount_factor = 0.99\n",
        "\n",
        "# Load the Environment\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=True) \n",
        "\n",
        "# Find the Optimal Policy\n",
        "optimal_policy = policy_iteration(env, discount_factor)\n",
        "\n",
        "print(' '.join([action_mapping[action] for action in optimal_policy]))\n",
        "\n",
        "# Apply the Optimal Policy\n",
        "scores, wins = run_episodes(env, optimal_policy, discount_factor, render=False)\n",
        "# Find the Average reward score\n",
        "print(\"Total number of Episodes = 1000\")\n",
        "print('Average reward = ', np.mean(scores))\n",
        "print(\"Total number of wins: \",wins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDuyUN4J6-kr",
        "outputId": "08cc745c-8df0-4d05-aceb-3afcaa42532d"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy-Iteration converged at step 7.\n",
            "← ↑ ↑ ↑ ← ← ← ← ↑ ↓ ← ← ← → ↓ ←\n",
            "Total number of Episodes = 1000\n",
            "Average reward =  0.732\n",
            "Total number of wins:  1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zroKlefE74_J"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2."
      ],
      "metadata": {
        "id": "sKAEW09sLsuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Create a Value Iteration function with the following parameters</h3>\n",
        "<ol>\n",
        "<li>environment: Initialized OpenAI gym environment object\n",
        "<li>discount_factor: MDP discount factor\n",
        "<li>theta:  A  threshold  of  a  value  function  change.  Once  the  update  to  value function is below this number\n",
        "<li>max_iterations: Maximum number of iterations\n",
        "</ol>"
      ],
      "metadata": {
        "id": "Z0gpC6k3CoIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_policy(v, discount_factor = 0.99):\n",
        "    # Initialize a policy for the state space\n",
        "    policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "    # Iterate through the state space\n",
        "    for s in range(env.observation_space.n):\n",
        "        q_sa = np.zeros(env.action_space.n)\n",
        "        for a in range(env.action_space.n):\n",
        "            for next_sr in env.P[s][a]:\n",
        "                # next_sr is a tuple of (probability, next state, reward, done)\n",
        "                p, s_, r, _ = next_sr\n",
        "                q_sa[a] += (p * (r + discount_factor * v[s_]))\n",
        "        # Update the policy with best possible action value for the state s\n",
        "        policy[s] = np.argmax(q_sa)\n",
        "    \n",
        "    # Return the (new) Updated Policy\n",
        "    return policy"
      ],
      "metadata": {
        "id": "yaogc20zDRdx"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, discount_factor = 0.99):\n",
        "    # Initialize the value-function\n",
        "    v = np.zeros(env.observation_space.n)\n",
        "    # Set variables\n",
        "    max_iterations = 10000\n",
        "    theta = 1e-20\n",
        "    for i in range(max_iterations):\n",
        "        # Store the value function before updating\n",
        "        prev_v = np.copy(v)\n",
        "        # Iterate through the State space\n",
        "        for s in range(env.observation_space.n):\n",
        "            # Find the Estimated Action value of all Actions for state s\n",
        "            q_sa = [sum([p*(r + discount_factor * prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.action_space.n)]\n",
        "            # Update the Value Function\n",
        "            v[s] = max(q_sa)\n",
        "\n",
        "        # Condition to break the loop\n",
        "        if (np.sum(np.fabs(prev_v - v)) <= theta):\n",
        "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
        "            break\n",
        "    # Return the value Function\n",
        "    return v"
      ],
      "metadata": {
        "id": "Eak-gK5VFmRo"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episodes(env, policy, discount_factor, render = False):\n",
        "    \"\"\" Runs an episode and return the total reward \"\"\"\n",
        "    # Set number of episodes\n",
        "    num_episode = 1000\n",
        "    max_steps = 100\n",
        "    episode_reward = []\n",
        "    wins = 0\n",
        "\n",
        "    # Iterate through the episodes\n",
        "    for i in range(num_episode):\n",
        "      # Reset the Environment\n",
        "      obs = env.reset()\n",
        "      # Initialize the variables\n",
        "      total_reward = 0\n",
        "      step_idx = 0\n",
        "      while step_idx < max_steps:\n",
        "          if render:\n",
        "              env.render()\n",
        "          # Take action based on policy\n",
        "          obs, reward, done , _ = env.step(int(policy[obs]))\n",
        "          # Update the cumulative reward\n",
        "          # total_reward += (discount_factor ** step_idx * reward)\n",
        "          total_reward += reward\n",
        "          step_idx += 1\n",
        "          # Check if reached Goal State\n",
        "          if done:\n",
        "              wins += 1\n",
        "              break\n",
        "      # Add the episode reward to a List\n",
        "      episode_reward.append(total_reward)\n",
        "\n",
        "    # Return list of rewards for 1000 episodes\n",
        "    return episode_reward, wins"
      ],
      "metadata": {
        "id": "LKl_Vei7HsA1"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discount_factor = 0.99\n",
        "\n",
        "# Load the Environment\n",
        "env_name  = 'FrozenLake-v1'\n",
        "env = gym.make(env_name, is_slippery=True)\n",
        "\n",
        "# Find the Optimal Value Functioin\n",
        "optimal_vf = value_iteration(env, discount_factor);\n",
        "# Obtain Policy based on this value function\n",
        "policy = extract_policy(optimal_vf, discount_factor)\n",
        "\n",
        "print(' '.join([action_mapping[action] for action in policy]))\n",
        "\n",
        "# Apply this policy and Evaluate\n",
        "value_scores, wins = run_episodes(env, policy, discount_factor, render=False)\n",
        "print(\"Using Value Iteration Function\")\n",
        "print(\"Total number of Episodes = 1000\")\n",
        "print('Average reward = ', np.mean(value_scores))\n",
        "print(\"Total number of wins: \",wins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1jXHxD7HXmO",
        "outputId": "bccd15e6-252a-4d53-8aa1-560091a6cc9e"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value-iteration converged at iteration# 996.\n",
            "← ↑ ↑ ↑ ← ← ← ← ↑ ↓ ← ← ← → ↓ ←\n",
            "Using Value Iteration Function\n",
            "Total number of Episodes = 1000\n",
            "Average reward =  0.741\n",
            "Total number of wins:  1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3."
      ],
      "metadata": {
        "id": "9L2Ce3SVL0AB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Compare  the number of  wins, average  return  after  1000  episodes and  comment  on which method performed better.</h3>"
      ],
      "metadata": {
        "id": "m1hGBTXMJIgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both methods are have similar average score over 1000 episodes<br>\n",
        "But we can see that the average rewards for Value Iteration Function is slightly better than Policy Iteration Function.<br>\n",
        "Policy Iteration function coverges faster with fewer iterations thereby reducing its computation costs and execution time\n"
      ],
      "metadata": {
        "id": "ciaF01D8M641"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iU-lj0Nqy0qv"
      },
      "execution_count": 98,
      "outputs": []
    }
  ]
}